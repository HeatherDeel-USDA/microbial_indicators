---
title: "Random Forest"
author: "Heather Deel"
date: "2023-08-07"
output: html_document
---

### Setup and packages
```{r setup, include=FALSE}
library(tidyverse)
geom_text_repel2 <- function(...) {
    layer <- ggrepel::geom_text_repel(...)
    layer$ggrepel <- TRUE
    class(layer) <- c("ggrepel", class(layer))
    return(layer)
}

ggplot_add.ggrepel <- function(object, plot, object_name) {
    if (any(do.call(c, lapply(plot$layer, function(x) x$ggrepel)))) {
        warning(
            "There is more than one ggrepel layers. ",
            "This may cause overlap of labels"
        )
    }
   # Optionally, one may modify `object` here.
    NextMethod("ggplot_add")
}
```

### Load and format 16S data
```{r}
### Hops.2018
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('../../picrust2_files/Hops.2018/EC/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('../../picrust2_files/Hops.2018/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# get other metadata
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# get qPCR data into func
# need to read in raw qPCR values
qPCR_data <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.qPCR.txt', sep = '\t')
qPCR_data <- qPCR_data %>% 
  filter(!is.na(qPCR))

func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hops2018_all <- merge(data.merged, otu, by = "SampleID")
hops2018_all <- as.data.frame(hops2018_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hops2018_ml <- hops2018_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "../../machine_learning/EC_RA/hops2018_ml_EC_RA.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hops2018_all <- merge(data.merged, otu, by = "SampleID")
hops2018_all <- as.data.frame(hops2018_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hops2018_ml <- hops2018_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "../../machine_learning/EC_RA_corr/hops2018_ml_EC_RA_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_Total_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_Total_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hops2018_all <- merge(data.merged, otu, by = "SampleID")
hops2018_all <- as.data.frame(hops2018_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hops2018_ml <- hops2018_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "../../machine_learning/EC_Total_corr/hops2018_ml_EC_Total_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=-sum(EC_RA*log(EC_RA)))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hops2018_all <- merge(data.merged, otu, by = "SampleID")
hops2018_all <- as.data.frame(hops2018_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hops2018_ml <- hops2018_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "../../machine_learning/EC_H/hops2018_ml_EC_H.RDS")



### Hops.ARS
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('../../picrust2_files/Hops.ARS/EC/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('../../picrust2_files/Hops.ARS/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# get other metadata
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# get qPCR data into func
# need to read in raw qPCR values
qPCR_data <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.qPCR.txt', sep = '\t')
qPCR_data <- qPCR_data %>% 
  filter(!is.na(qPCR))

func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hopsARS_all <- merge(data.merged, otu, by = "SampleID")
hopsARS_all <- as.data.frame(hopsARS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hopsARS_ml <- hopsARS_all[,c(1,95,152:2390)]

saveRDS(hopsARS_ml, "../../machine_learning/EC_RA/hopsARS_ml_EC_RA.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hopsARS_all <- merge(data.merged, otu, by = "SampleID")
hopsARS_all <- as.data.frame(hopsARS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hopsARS_ml <- hopsARS_all[,c(1,95,152:2390)]

saveRDS(hopsARS_ml, "../../machine_learning/EC_RA_corr/hopsARS_ml_EC_RA_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_Total_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_Total_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hopsARS_all <- merge(data.merged, otu, by = "SampleID")
hopsARS_all <- as.data.frame(hopsARS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hopsARS_ml <- hopsARS_all[,c(1,95,152:2390)]

saveRDS(hopsARS_ml, "../../machine_learning/EC_Total_corr/hopsARS_ml_EC_Total_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=-sum(EC_RA*log(EC_RA)))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hopsARS_all <- merge(data.merged, otu, by = "SampleID")
hopsARS_all <- as.data.frame(hopsARS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hopsARS_ml <- hopsARS_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "../../machine_learning/EC_H/hopsARS_ml_EC_H.RDS")



### NRCS
# split NRCS data into two to keep R from crashing
func <- read.delim('../../picrust2_files/NRCS/EC/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('../../picrust2_files/NRCS/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# get other metadata
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# get qPCR data into func
# need to read in raw qPCR values
qPCR_data <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.qPCR.txt', sep = '\t')
qPCR_data <- qPCR_data %>% 
  filter(!is.na(qPCR))

func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
NRCS_all <- merge(data.merged, otu, by = "SampleID")
NRCS_all <- as.data.frame(NRCS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
NRCS_ml <- NRCS_all[,c(1,95,152:2390)]

saveRDS(NRCS_ml, "../../machine_learning/EC_RA/NRCS_ml_EC_RA.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
NRCS_all <- merge(data.merged, otu, by = "SampleID")
NRCS_all <- as.data.frame(NRCS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
NRCS_ml <- NRCS_all[,c(1,95,152:2390)]

saveRDS(NRCS_ml, "../../machine_learning/EC_RA_corr/NRCS_ml_EC_RA_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_Total_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_Total_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
NRCS_all <- merge(data.merged, otu, by = "SampleID")
NRCS_all <- as.data.frame(NRCS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
NRCS_ml <- NRCS_all[,c(1,95,152:2390)]

saveRDS(NRCS_ml, "../../machine_learning/EC_Total_corr/NRCS_ml_EC_Total_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=-sum(EC_RA*log(EC_RA)))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
NRCS_all <- merge(data.merged, otu, by = "SampleID")
NRCS_all <- as.data.frame(NRCS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
NRCS_ml <- NRCS_all[,c(1,95,152:2390)]

saveRDS(NRCS_ml, "../../machine_learning/EC_H/NRCS_ml_EC_H.RDS")



### Rangeland
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('../../picrust2_files/Rangeland/EC/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('../../picrust2_files/Rangeland/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# get other metadata
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# get qPCR data into func
# need to read in raw qPCR values
qPCR_data <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.qPCR.txt', sep = '\t')
qPCR_data <- qPCR_data %>% 
  filter(!is.na(qPCR))

func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
Rangeland_all <- merge(data.merged, otu, by = "SampleID")
Rangeland_all <- as.data.frame(Rangeland_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
Rangeland_ml <- Rangeland_all[,c(1,95,152:2390)]

saveRDS(Rangeland_ml, "../../machine_learning/EC_RA/Rangeland_ml_EC_RA.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_RA_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
Rangeland_all <- merge(data.merged, otu, by = "SampleID")
Rangeland_all <- as.data.frame(Rangeland_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
Rangeland_ml <- Rangeland_all[,c(1,95,152:2390)]

saveRDS(Rangeland_ml, "../../machine_learning/EC_RA_corr/Rangeland_ml_EC_RA_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_Total_corr <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(EC_Total_corr))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
Rangeland_all <- merge(data.merged, otu, by = "SampleID")
Rangeland_all <- as.data.frame(Rangeland_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
Rangeland_ml <- Rangeland_all[,c(1,95,152:2390)]

saveRDS(Rangeland_ml, "../../machine_learning/EC_Total_corr/Rangeland_ml_EC_Total_corr.RDS")


# we imported un-normalized counts, so normalizing here
func$EC_RA <- func$taxon_rel_abun / 100

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=-sum(EC_RA*log(EC_RA)))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
Rangeland_all <- merge(data.merged, otu, by = "SampleID")
Rangeland_all <- as.data.frame(Rangeland_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
Rangeland_ml <- Rangeland_all[,c(1,95,152:2390)]

saveRDS(Rangeland_ml, "../../machine_learning/EC_H/Rangeland_ml_EC_H.RDS")




### final formatting of ml_16S object
# merge ml dfs for Hops.2018 Hops.ARS, NRCS1, NRCS2, and Rangeland
hops2018_ml <- readRDS("../../machine_learning/EC_RA/hops2018_ml_EC_RA.RDS")
hopsARS_ml <- readRDS("../../machine_learning/EC_RA/hopsARS_ml_EC_RA.RDS")
NRCS_ml <- readRDS("../../machine_learning/EC_RA/NRCS_ml_EC_RA.RDS")
rangeland_ml <- readRDS("../../machine_learning/EC_RA/rangeland_ml_EC_RA.RDS")

ml_1 <- merge(hops2018_ml, hopsARS_ml, all = TRUE)
ml_2 <- merge(NRCS_ml, ml_1, all = TRUE)
ml_all <- merge(rangeland_ml, ml_2, all = TRUE)

# actually need to add back in other metadata columns - would take too long to reprocess and save everything
# reread in files from local environment
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# merge ml_all2 with data.merged
ml_all_meta <- merge(ml_all, data.merged, by = c("SampleID","Overall"))

# get rid of duplicate "Overall" column
#ml_all_meta <- ml_all_meta[c(1,3:ncol(ml_all_meta))]
#colnames(ml_all_meta)[2539] = "Overall"

# going to assume that NAs are 0 (functions not found in one data set or another)
# change NAs to 0
ml_all_meta[is.na(ml_all_meta)] <- 0

# check for any NAs
sum(is.na(ml_all_meta))

# NRCS 346 duplicated for some reason. One row is all zeros. Get rid of row 315
#ml_all_meta <- ml_all_meta[c(1:314,316:537),]

# data ready
saveRDS(ml_all_meta, '../../machine_learning/EC_RA/ml_EC_RA.RDS')


# next set
hops2018_ml <- readRDS("../../machine_learning/EC_RA_corr/hops2018_ml_EC_RA_corr.RDS")
hopsARS_ml <- readRDS("../../machine_learning/EC_RA_corr/hopsARS_ml_EC_RA_corr.RDS")
NRCS_ml <- readRDS("../../machine_learning/EC_RA_corr/NRCS_ml_EC_RA_corr.RDS")
rangeland_ml <- readRDS("../../machine_learning/EC_RA_corr/rangeland_ml_EC_RA_corr.RDS")

ml_1 <- merge(hops2018_ml, hopsARS_ml, all = TRUE)
ml_2 <- merge(NRCS_ml, ml_1, all = TRUE)
ml_all <- merge(rangeland_ml, ml_2, all = TRUE)

# actually need to add back in other metadata columns - would take too long to reprocess and save everything
# reread in files from local environment
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# merge ml_all2 with data.merged
ml_all_meta <- merge(ml_all, data.merged, by = c("SampleID","Overall"))

# get rid of duplicate "Overall" column
#ml_all_meta <- ml_all_meta[c(1,3:2596)]
#colnames(ml_all_meta)[2539] = "Overall"

# going to assume that NAs are 0 (functions not found in one data set or another)
# change NAs to 0
ml_all_meta[is.na(ml_all_meta)] <- 0

# check for any NAs
sum(is.na(ml_all_meta))

# NRCS 346 duplicated for some reason. One row is all zeros. Get rid of row 315
#ml_all_meta <- ml_all_meta[c(1:314,316:537),]

# data ready
saveRDS(ml_all_meta, '../../machine_learning/EC_RA_corr/ml_EC_RA_corr.RDS')




hops2018_ml <- readRDS("../../machine_learning/EC_Total_corr/hops2018_ml_EC_Total_corr.RDS")
hopsARS_ml <- readRDS("../../machine_learning/EC_Total_corr/hopsARS_ml_EC_Total_corr.RDS")
NRCS_ml <- readRDS("../../machine_learning/EC_Total_corr/NRCS_ml_EC_Total_corr.RDS")
rangeland_ml <- readRDS("../../machine_learning/EC_Total_corr/rangeland_ml_EC_Total_corr.RDS")

ml_1 <- merge(hops2018_ml, hopsARS_ml, all = TRUE)
ml_2 <- merge(NRCS_ml, ml_1, all = TRUE)
ml_all <- merge(rangeland_ml, ml_2, all = TRUE)

# actually need to add back in other metadata columns - would take too long to reprocess and save everything
# reread in files from local environment
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# merge ml_all2 with data.merged
ml_all_meta <- merge(ml_all, data.merged, by = c("SampleID","Overall"))

# get rid of duplicate "Overall" column
#ml_all_meta <- ml_all_meta[c(1,3:2596)]
#colnames(ml_all_meta)[2539] = "Overall"

# going to assume that NAs are 0 (functions not found in one data set or another)
# change NAs to 0
ml_all_meta[is.na(ml_all_meta)] <- 0

# check for any NAs
sum(is.na(ml_all_meta))

# NRCS 346 duplicated for some reason. One row is all zeros. Get rid of row 315
#ml_all_meta <- ml_all_meta[c(1:314,316:537),]

# data ready
saveRDS(ml_all_meta, '../../machine_learning/EC_Total_corr/ml_EC_Total_corr.RDS')




hops2018_ml <- readRDS("../../machine_learning/EC_H/hops2018_ml_EC_H.RDS")
hopsARS_ml <- readRDS("../../machine_learning/EC_H/hopsARS_ml_EC_H.RDS")
NRCS_ml <- readRDS("../../machine_learning/EC_H/NRCS_ml_EC_H.RDS")
rangeland_ml <- readRDS("../../machine_learning/EC_H/rangeland_ml_EC_H.RDS")

ml_1 <- merge(hops2018_ml, hopsARS_ml, all = TRUE)
ml_2 <- merge(NRCS_ml, ml_1, all = TRUE)
ml_all <- merge(rangeland_ml, ml_2, all = TRUE)

# actually need to add back in other metadata columns - would take too long to reprocess and save everything
# reread in files from local environment
data.sampleID <- read.delim('../../metadata/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')
data.pred.SHMI <- readRDS('../../metadata/data.pred.SHMI.RDS')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# merge ml_all2 with data.merged
ml_all_meta <- merge(ml_all, data.merged, by = c("SampleID","Overall"))

# get rid of duplicate "Overall" column
#ml_all_meta <- ml_all_meta[c(1,3:2596)]
#colnames(ml_all_meta)[2539] = "Overall"

# going to assume that NAs are 0 (functions not found in one data set or another)
# change NAs to 0
ml_all_meta[is.na(ml_all_meta)] <- 0

# check for any NAs
sum(is.na(ml_all_meta))

# NRCS 346 duplicated for some reason. One row is all zeros. Get rid of row 315
#ml_all_meta <- ml_all_meta[c(1:314,316:537),]

# data ready
saveRDS(ml_all_meta, '../../machine_learning/EC_H/ml_EC_H.RDS')

```


```{r}
library(M3C)
library(vegan)

P <- readRDS('../../machine_learning/EC_RA_corr/ml_EC_RA_corr.RDS')

gibbs <- read.csv('../../picrust2_files/GIBBs.EC.numbers.csv')
gibbs <- gibbs %>%
  filter(Missing != 'x')

data <- P %>%
  select(gibbs$EC)

meta <- P

library(M3C)
#tsne_out <- Rtsne::Rtsne(data, perplexity=15, max_iter=1000)
#plot(tsne_out$Y)

p1 <- M3C::tsne(t(data), labels=FALSE, dotsize = 0, seed=1234, perplex=15) + theme_bw() +
  geom_point(aes(fill=meta$ClimateZ, shape=meta$Current_Land_Use), size=4) +
  scale_shape_manual(values=c(21,22,23,24)) +
  guides(size="none",
         fill = guide_legend(override.aes = list(size = 4, shape=21))) +
  labs(fill="ClimateZ", shape="Land Use", x="tSNE 1", y="tSNE 2")
p
ggsave(paste0("../../dan_out_2/", "All_GIBBs_RA_corr_tSNE.png"), device='png', dpi=600, height=4, width=6)


# ord <- dbrda(vegdist(data, 'bray') ~ 1)
# 
# ### get eigenvalues and proportion explained
# ev <- eigenvals(ord)
# Axis1_exp = round(ev[1] / sum(ev) * 100, 1)
# Axis2_exp = round(ev[2] / sum(ev) * 100, 1)
# 
# ### get PC axes
# axes <- as.data.frame(scores(ord, display="sites", choices=c(1:2)))
# names(axes) <- c("Axis1", "Axis2")
# axes <- cbind(meta, axes)
# 
# ### create ggplot
# colVec1 <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#A65628", "grey")
# 
# mult <- 1
# 
# axes$ClimateZ <- factor(axes$ClimateZ)
# axes$Current_Land_Use <- factor(axes$Current_Land_Use)
# 
# #sub <- axes[!(axes$ClimateZ %in% c('BSk', 'Csb')),]
# 
# p <- ggplot(data=axes, aes(x=Axis1, y=Axis2))
# p <- p + geom_point(aes(fill=Current_Land_Use), color='black', size=6, shape=21)
# p <- p + scale_fill_manual(values=colVec1, name='Current_Land_Use')
# #p <- p + stat_ellipse(geom = "path", type="t", level=0.95, aes(color=Current_Land_Use), show.legend=FALSE)
# p <- p + scale_color_manual(values=colVec1)
# p <- p + guides(size="none",
#                 fill = guide_legend(override.aes = list(size = 5, shape=21)))
# p <- p + theme_bw()
# p

```
# Predict clay, climate, DNA
```{r}
# libraries
library(dplyr)
library(party)
library(tidyverse)
library(Boruta)
library(moreparty)
library(permimp)


myVars <- c('clay', 'DNA')
myDatas <- c('TAX_RA', 'EC_RA_corr', 'GIBBs_RA_corr')

for (myData in myDatas) {
    ml_EC_16S <- readRDS(paste0('../../machine_learning/', myData, '/ml_', myData, '.RDS'))
    
    # scale selected variables
    to_scale <- c('ph', 'p', 'k', 'mg', 'fe', 'mn', 'zn', 'DNA')
    ml_EC_16S <- ml_EC_16S %>%
      mutate_at(to_scale, funs(c(scale(.))))

    for (myVar in myVars) {
    print(paste0("Variable:", myVar))
  
    if (myData %in% c("TAX_RA", "TAX_Total")) {
      data <- ml_EC_16S[,c(152:ncol(ml_EC_16S))]
      df.myVar <- as.vector(ml_EC_16S[,myVar])
      data <- cbind(df.myVar, data)
      names(data)[1] <- myVar
    } else {
      data <- ml_EC_16S %>% select(grep("EC:", colnames(ml_EC_16S)))
      df.myVar <- as.vector(ml_EC_16S[,myVar])
      data <- cbind(df.myVar, data)
      names(data)[1] <- myVar
    }
      
    if (myVar == 'ClimateZ') {
      data$ClimateZ <- factor(data$ClimateZ)
    }
    
    # format so : and . are replaced by _ (for varimp)
    names(data) <- gsub(":","_", names(data))
    names(data) <- gsub("\\.","_", names(data))
  
    if (myData %in% c("TAX_RA", "TAX_Total", 'EC_RA_corr', 'EC_Total_corr')) { 
      # Select using Boruta
      myFormula <- as.formula(paste0(myVar, ' ~ .'))
      Boruta.res <- Boruta(myFormula, data=data)
      myFormula <- getNonRejectedFormula(Boruta.res)
      keep_X <- names(Boruta.res$finalDecision[Boruta.res$finalDecision != "Rejected"])
    } else {
      # GIBBs enzymes 
      gibbs <- read.csv('../../picrust2_files/GIBBs.EC.numbers.csv')
      gibbs <- gibbs %>%
        filter(Missing != 'x')
      gibbs$EC <- gsub(":","_", gibbs$EC)
      gibbs$EC <- gsub("\\.","_", gibbs$EC)
      
      keep_X <- c(gibbs$EC)
      myFormula <- as.formula(paste0(myVar, "~ ."))
    }
    
    final <- data %>%
      select(all_of(keep_X), all_of(myVar))
    
    for (myRun in seq(1,25,1)) {
      print(paste0("Starting run: ", myRun))
      
      # split into train and test (4/5 proportion)
      final$id <- 1:nrow(final)
      train <- final %>% dplyr::sample_frac(0.80)
      test <- dplyr::anti_join(final, train, by = 'id')
      
      # get rid of id columns
      train <- train %>% select(-id)
      test <- test %>% select(-id)
      
      # cforest on training data
      my_cforest_control <- cforest_control(teststat = "quad",
          testtype = "Univ", mincriterion = 0, ntree = 500, 
          mtry = ceiling(length(keep_X)/3),
          replace = FALSE)
      
      cf.train <- cforest(myFormula, data = train,
                        controls = my_cforest_control)
      
      cf.pred <- predict(cf.train, newdata = test, OOB = TRUE, type = "response")
      
      # observed vs predicted
      colnames(cf.pred)[1] <- "pred"
      cf.pred <- data.frame(cf.pred)
      cf.pred <- rownames_to_column(cf.pred, var = "id")
      obs <- data.frame(test[,myVar])
      colnames(obs)[1] <- "obs"
      cf.pvso <- cbind(cf.pred, obs)
  
      # save R^2 and p-values to a file
      res.lm <- lm(obs ~ pred, data = cf.pvso)
      r2_val <- summary(res.lm)$adj.r.squared
      p_val <- summary(res.lm)$coef[2,4]
      
      write.table(cbind(myRun, r2_val, p_val), 
                  file = paste0("../../dan_out_2/", myVar, "_", myData, "_stats.csv"), 
                  col.names = FALSE,
                  append = TRUE, sep = ",", row.names = FALSE)
      
      # variable importances
      imp <- permimp::permimp(cf.train, nperm=1, OOB=TRUE, scaled=FALSE,
                              conditional=FALSE, threshold=0.2, asParty=FALSE,
                              thresholdDiagnostics = FALSE, progressBar = TRUE)

      imp <- as.data.frame(imp$values)
      names(imp) <- 'vimp'
      imp <- imp %>% arrange(desc(vimp))
      imp <- tibble::rownames_to_column(imp, "EC")
      imp$Run <- myRun

      write.table(imp,
                 file = paste0("../../dan_out_2/", myVar, "_", myData, "_varimp.csv"),
                 col.names=FALSE,
                 append = TRUE, sep = ",", row.names = FALSE)
    }
  }
}

```



```{r}
# libraries
library(dplyr)
library(party)
library(tidyverse)
library(Boruta)
library(moreparty)
library(permimp)


#myVars <- c('ace', 'SOM', 'activeC', 'resp', 'agg_stab', 'water_cap', 
#            'ph', 'p', 'k', 'mg', 'fe', 'mn', 'zn')
myVars <- c('clay', 'DNA')
myDatas <- c('TAX_RA', 'EC_RA_corr', 'GIBBs_RA_corr')

for (myData in myDatas) {
    ml_EC_16S <- readRDS(paste0('../../machine_learning/', myData, '/ml_', myData, '.RDS'))
    
    # scale selected variables
    to_scale <- c('ph', 'p', 'k', 'mg', 'fe', 'mn', 'zn', 'DNA')
    ml_EC_16S <- ml_EC_16S %>%
      mutate_at(to_scale, funs(c(scale(.))))

    for (myVar in myVars) {
    print(paste0("Variable:", myVar))
  
    if (myData %in% c("TAX_RA", "TAX_Total")) {
      data <- ml_EC_16S[,c(152:ncol(ml_EC_16S))]
      df.myVar <- as.vector(ml_EC_16S[,myVar])
      df.climate <- as.vector(ml_EC_16S[,'ClimateZ'])
      df.clay <- as.vector(ml_EC_16S[,'clay'])
      df.dna <- as.vector(ml_EC_16S[,'DNA'])
      data <- cbind(df.myVar, df.climate, df.clay, df.dna, data)
      names(data)[1] <- myVar
      names(data)[2] <- 'ClimateZ'
      names(data)[3] <- 'clay'
      names(data)[4] <- 'DNA'
    } else {
      data <- ml_EC_16S %>% select(grep("EC:", colnames(ml_EC_16S)))
      df.myVar <- as.vector(ml_EC_16S[,myVar])
      df.climate <- as.vector(ml_EC_16S[,'ClimateZ'])
      df.clay <- as.vector(ml_EC_16S[,'clay'])
      df.dna <- as.vector(ml_EC_16S[,'DNA'])
      data <- cbind(df.myVar, df.climate, df.clay, df.dna, data)
      #data <- cbind(df.myVar, df.dna, data)
      names(data)[1] <- myVar
      names(data)[2] <- 'ClimateZ'
      names(data)[3] <- 'clay'
      names(data)[4] <- 'DNA'
    }
    
    # format so : and . are replaced by _ (for varimp)
    names(data) <- gsub(":","_", names(data))
    names(data) <- gsub("\\.","_", names(data))
  
    if (myData %in% c("TAX_RA", "TAX_Total", 'EC_RA_corr', 'EC_Total_corr')) { 
      # Select using Boruta
      myFormula <- as.formula(paste0(myVar, ' ~ .'))
      Boruta.res <- Boruta(myFormula, data=data)
      myFormula <- getNonRejectedFormula(Boruta.res)
      keep_X <- names(Boruta.res$finalDecision[Boruta.res$finalDecision != "Rejected"])
    } else {
      # GIBBs enzymes 
      gibbs <- read.csv('../../picrust2_files/GIBBs.EC.numbers.csv')
      gibbs <- gibbs %>%
        filter(Missing != 'x')
      gibbs$EC <- gsub(":","_", gibbs$EC)
      gibbs$EC <- gsub("\\.","_", gibbs$EC)
      
      keep_X <- c(gibbs$EC, 'ClimateZ', 'clay', 'DNA')
      myFormula <- as.formula(paste0(myVar, "~ ."))
    }
    
    final <- data %>%
      select(all_of(keep_X), all_of(myVar))
    
    if ('ClimateZ' %in% keep_X) {
      final$ClimateZ <- factor(final$ClimateZ)
    }
    
    if ('clay' %in% keep_X) {
      final$clay <- as.numeric(final$clay)
    }
  
    for (myRun in seq(1,25,1)) {
      print(paste0("Starting run: ", myRun))
      
      # split into train and test (4/5 proportion)
      final$id <- 1:nrow(final)
      train <- final %>% dplyr::sample_frac(0.80)
      test <- dplyr::anti_join(final, train, by = 'id')
      
      # get rid of id columns
      train <- train %>% select(-id)
      test <- test %>% select(-id)
      
      # cforest on training data
      my_cforest_control <- cforest_control(teststat = "quad",
          testtype = "Univ", mincriterion = 0, ntree = 500, 
          mtry = ceiling(length(keep_X)/3),
          replace = FALSE)
      
      cf.train <- cforest(myFormula, data = train,
                        controls = my_cforest_control)
      
      cf.pred <- predict(cf.train, newdata = test, OOB = TRUE, type = "response")
      
      # observed vs predicted
      colnames(cf.pred)[1] <- "pred"
      cf.pred <- data.frame(cf.pred)
      cf.pred <- rownames_to_column(cf.pred, var = "id")
      obs <- data.frame(test[,myVar])
      colnames(obs)[1] <- "obs"
      cf.pvso <- cbind(cf.pred, obs)
  
      # save R^2 and p-values to a file
      res.lm <- lm(obs ~ pred, data = cf.pvso)
      r2_val <- summary(res.lm)$adj.r.squared
      p_val <- summary(res.lm)$coef[2,4]
      
      write.table(cbind(myRun, r2_val, p_val), 
                  file = paste0("../../dan_out_2/", myVar, "_", myData, "_stats.csv"), 
                  col.names = FALSE,
                  append = TRUE, sep = ",", row.names = FALSE)
      
      # variable importances
      imp <- permimp::permimp(cf.train, nperm=1, OOB=TRUE, scaled=FALSE,
                              conditional=FALSE, threshold=0.2, asParty=FALSE,
                              thresholdDiagnostics = FALSE, progressBar = TRUE)

      imp <- as.data.frame(imp$values)
      names(imp) <- 'vimp'
      imp <- imp %>% arrange(desc(vimp))
      imp <- tibble::rownames_to_column(imp, "EC")
      imp$Run <- myRun

      write.table(imp,
                 file = paste0("../../dan_out_2/", myVar, "_", myData, "_varimp.csv"),
                 col.names=FALSE,
                 append = TRUE, sep = ",", row.names = FALSE)
    }
  }
}

```


```{r}
library(paletteer)
gibbs <- read.csv('../../picrust2_files/GIBBs.EC.numbers.csv')
gibbs <- gibbs %>%
  filter(Missing != 'x')

myVars <- c('ace', 'SOM', 'activeC', 'resp', 'agg_stab', 'water_cap', 'ph', 'p', 'k', 'mg', 'fe', 'mn', 'zn')
myData <- 'GIBBs_RA_corr'

counter <- 1
for (myVar in myVars) {
  df <- read.csv(paste0("../../dan_out_2/", myVar, "_", myData, "_varimp.csv"), header=FALSE)
  names(df) <- c('Predictor', 'VIP', 'Run')
  df$Predictor <- gsub("EC_","EC:", df$Predictor)
  df$Predictor <- gsub("_","\\.", df$Predictor)
  gDF <- df %>%
    left_join(gibbs, by=c("Predictor"="EC")) %>% 
    mutate(Predictor=fct_reorder(Predictor, VIP, .fun=median, .desc=TRUE)) # sort levels by median VIP
  
  # filter out poor indicators
  temp <- gDF %>% group_by(Predictor) %>%
    summarise(Var = median(VIP)) %>%
    top_n(20, Var) 
  
  gDF <- gDF %>%
    filter(Predictor %in% temp$Predictor)
  
  #colors <- paletteer_d(`"ggsci::category20_d3"`)
  colors <- c("Biocontrol" = "#1F77B4FF", 
              "C cycling" = "#FF7F0EFF",
              "C/N cycling" = "#2CA02CFF",
              "N cycling" = "#D62728FF", 
              "P cycling" = "#9467BDFF", 
              "S cycling" = "#8C564BFF",
              "Stess" = "#E377C2FF",
              "Siderophore" = "#7F7F7FFF",
              "Other" = "white")
  means <- aggregate(VIP ~ Predictor, gDF, median)
  means$Category <- 'Other'
  means$VIP <- round(means$VIP, 3)
  p <- ggplot(gDF, aes(x=Predictor, y=VIP, fill=Category)) +
    scale_fill_manual(values=colors, na.value="white") +
    geom_boxplot() + 
    geom_point(aes(fill=Category), shape=21) + 
    #stat_summary(fun=mean, colour="black", geom="point", 
    #            shape=18, size=3, show.legend=FALSE) + 
    geom_text(data = means, aes(label = VIP, y = -0.025), size=2.8, fontface='italic', angle=30) +
    theme_bw() +
    theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5)) +
    labs(x="", y="Variable Importance")
  ggsave(paste0("../../dan_out_2/", myVar, "_", myData, "_varimp.png"), device='png', dpi=600, height=6, width=8)
  
  means <- df %>%
    group_by(Predictor) %>%
    summarize(!!myVar := median(VIP)) %>%
    mutate_if(is.numeric, list(~ ifelse(. < 0, 0, .))) %>%
    mutate_if(is.numeric, list(~ ./sum(.)))
  
  if (counter == 1) {
    meansDF.wide <- means
    means$myVar <- myVar
    names(means) <- c('Predictor', 'VIP', 'myVar')
    meansDF.long <- means
  } else {
    meansDF.wide <- meansDF.wide %>%
      full_join(means, by='Predictor')
    means$myVar <- myVar
    names(means) <- c('Predictor', 'VIP', 'myVar')
    meansDF.long <- rbind(meansDF.long, means)
  }
  counter <- counter + 1
}

meansDF.wide
write.csv(meansDF.wide, paste0("../../dan_out_2/", "meansDF.wide.", myData, ".csv"))
meansDF.long

library(ggdendro)
names(meansDF.wide) <- c('Predictor', 'ACE Protein', 'Soil Organic Matter', 'Active C', 'Respiration', 
                        'Aggregate Stability', 'Water Capacity', 'pH', 'P', 'K', 'Mg', 'Fe', 'Mn', 'Zn')
data <- meansDF.wide %>%
  select(-Predictor)
data[is.na(data)] <- 0

data <- t(data)
colnames(data) <- meansDF.wide$Predictor

dist <- vegan::vegdist(data, method='bray', binary=FALSE)
dhc <- as.dendrogram(hclust(dist))

p <- ggdendrogram(dhc, rotate = TRUE) + 
  ylab('Bray-Curtis Dissimilarity') + 
  xlab('') 
p
ggsave(paste0("../../dan_out_2/", "All_", myData, "_dendro.png"), device='png', dpi=600, height=4, width=6)

library(vegan)
ord <- dbrda(dist ~ 1)
sppscores(ord) <- data

#get values for plotting
q <- ordiplot(ord, choices=c(1,2), scaling=3) # scaling = 1 (sites), 2 (species), 3 (symmetric)

## get biplot
biplot <- data.frame(q$biplot)
biplot <- biplot * attr(q,"const") # biplot scores are not scaled by default, multiply by scaling factor

# extract ordination data
species <- data.frame(q$species) 
names(species) <- c("Axis1", "Axis2")
species$lab <- row.names(species)
species[abs(species[,1]) < 0.2 & abs(species[,2]) < 0.2,] <- NA
species$colors <- 'black'
species$lab <- gsub('\\.', ' ', species$lab)

### get eigenvalues and proportion explained
ev <- eigenvals(ord)
Axis1_exp = round(ev[1] / sum(ev) * 100, 1)
Axis2_exp = round(ev[2] / sum(ev) * 100, 1)

### get PC axes
axes <- as.data.frame(q$sites)
names(axes) <- c("Axis1", "Axis2")
axes$Indicator <- row.names(axes)
axes$Indicator <- gsub("\\.", " ", axes$Indicator)

colors <- paletteer_d(`"ggsci::category20_d3"`)
p <- ggplot(data=axes, aes(x=Axis1, y=Axis2)) +
  geom_point(aes(fill=Indicator), size=5, color='black', shape=21) +
  scale_fill_manual(values=colors) +
  labs(fill='', color=NULL, shape='',
       x=paste0('MDS 1 (', Axis1_exp, '%)'), 
       y=paste0('MDS 2 (', Axis2_exp, '%)')) +
  theme_bw() +
  geom_segment(data=species, inherit.aes=F,
               aes(x=0, xend=Axis1, y=0, yend=Axis2),
               color='black', arrow = arrow(length = unit(0.1, "cm"))) +
  geom_text_repel2(data = species, inherit.aes=F,
                           aes(x=Axis1, y=Axis2, label=lab),
                           segment.size=0.25, segment.linetype=1, segment.color='grey',
                           color='black', size=3, direction='both', max.overlaps=50) 
p
ggsave(paste0("../../dan_out_2/", "All_", myData, "_PCoA.png"), device='png', dpi=600, height=4, width=6)
```



```{r}
library(microeco)
library(meconetcomp)
library(magrittr)
library(ggraph)
library(ggplot2)
library(file2meco)
library(phyloseq)

myData <- "EC_RA_corr"
ml_EC_16S <- readRDS(paste0('../../machine_learning/', myData, '/ml_', myData, '.RDS'))

data <- ml_EC_16S %>% select(grep("EC:", colnames(ml_EC_16S)))
data <- data[,colSums(data) > 0]
data <- data[, !sapply(data, function(x) { sd(x) == 0} )]
OTU <- otu_table(data, taxa_are_rows=FALSE)
sample_names(OTU) <- ml_EC_16S$SampleID

taxa <- read.csv('../../picrust2_files/All_EC_classes.csv', row.names='Name')
TAX <- tax_table(taxa)
taxa_names(TAX) <- row.names(taxa)
colnames(TAX) <- c('kingdom', 'phylum', 'class', 'order')

SAMP <- sample_data(ml_EC_16S[,c('SampleID', 'State', 'Site')])
sample_names(SAMP) <- ml_EC_16S$SampleID

P <- merge_phyloseq(SAMP, OTU, TAX)

EC.meco <- phyloseq2meco(P)

t1 <- clone(EC.meco)

t1 <- trans_network$new(dataset=EC.meco, cor_method="spearman", filter_thres = 0.001)
t1$cal_network(COR_p_thres = 0.01, COR_cut=0.7, add_taxa_name='order') # 0.69
t1$cal_module(method ='cluster_fast_greedy')
t1$cal_eigen()
t1$get_node_table(node_roles=TRUE) 
t1$get_edge_table()

colors <- rep(paletteer::paletteer_d("ggsci::default_igv", 51),3)
network <- t1$res_network
network_layout <- create_layout(network, layout = 'fr')
node_table <- t1$res_node_table
node_table$name <- row.names(node_table)
use_network_layout <- dplyr::left_join(network_layout, node_table, by='name')

g <- ggraph(use_network_layout) +
  geom_edge_link(aes(col = label, width = weight), alpha = 0.8) +
  geom_node_point(aes(fill = module.x, size = degree), shape = 21) +
  scale_fill_manual(values = colors, name = 'Module') +
  scale_edge_width(range = c(0.5, 2)) +
  theme_void() +
  theme(legend.position='none')
g
ggsave('network.pdf')
ggsave(paste0("../../dan_out_2/", "All_", myData, "_network.png"), device='png', dpi=600, height=8, width=11)


t1$plot_taxa_roles(use_type = 1)


```