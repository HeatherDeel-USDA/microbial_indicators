---
title: "16S_EC_loops"
author: "Heather Deel"
date: "2023-10-02"
output: html_document
---

### Setup and packages
```{r setup, include=FALSE}
library(tidymodels)
library(tidyverse)
library(workflows)
library(tune)
library(metagMisc)
library(ranger)
library(randomForest)
```

### Predict CASH rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_CASH <- ml_EC_16S[,c(2539,2:2445)]

# filter NAs
ml_EC_16S_CASH <- ml_EC_16S_CASH %>%
  filter(!is.na(Overall))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/CASH_model_results/ml_EC_16S_CASH_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(1)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_CASH, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting CASH rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(Overall ~ ., data = ml_EC_16S_CASH)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 1834, trees = 500, min_n = 5) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/CASH_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  CASH_EC_lm <- lm(Overall ~ .pred, data = test_predictions)
  p1 <- ggplot(CASH_EC_lm$model, aes(x = Overall, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(CASH_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(CASH_EC_lm)$coef[2,4], 2)),
                       x = "Observed CASH Rating", y = "Predicted CASH Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(CASH_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/CASH_model_results/CASH_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(CASH_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/CASH_model_results/CASH_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/CASH_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_CASH)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/CASH_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/CASH_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict SHMI2_rating SHMI rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_SHMI <- ml_EC_16S[,c(2594,2:2445)]

# filter NAs
ml_EC_16S_SHMI <- ml_EC_16S_SHMI %>%
  filter(!is.na(SHMI2_rating))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/SHMI_model_results/ml_EC_16S_SHMI_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(2)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_SHMI, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting SHMI rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(SHMI2_rating ~ ., data = ml_EC_16S_SHMI)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 1834, trees = 250, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/SHMI_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  SHMI_EC_lm <- lm(SHMI2_rating ~ .pred, data = test_predictions)
  p1 <- ggplot(SHMI_EC_lm$model, aes(x = SHMI2_rating, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(SHMI_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(SHMI_EC_lm)$coef[2,4], 2)),
                       x = "Observed SHMI Rating", y = "Predicted SHMI Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(SHMI_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/SHMI_model_results/SHMI_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(SHMI_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/SHMI_model_results/SHMI_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/SHMI_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_SHMI)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/SHMI_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/SHMI_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict SEMWISE rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_SEMWISE <- ml_EC_16S[,c(2557,2:2445)]

# filter NAs
ml_EC_16S_SEMWISE <- ml_EC_16S_SEMWISE %>%
  filter(!is.na(SH_rating))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/SEMWISE_model_results/ml_EC_16S_SEMWISE_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(3)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_SEMWISE, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting SEMWISE rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(SH_rating ~ ., data = ml_EC_16S_SEMWISE)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  SEMWISE_EC_lm <- lm(SH_rating ~ .pred, data = test_predictions)
  p1 <- ggplot(SEMWISE_EC_lm$model, aes(x = SH_rating, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(SEMWISE_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(SEMWISE_EC_lm)$coef[2,4], 2)),
                       x = "Observed SEMWISE Rating", y = "Predicted SEMWISE Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(SEMWISE_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/SEMWISE_model_results/SEMWISE_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(SEMWISE_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/SEMWISE_model_results/SEMWISE_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_SEMWISE)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/SEMWISE_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict ROOT rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_ROOT <- ml_EC_16S[,c(2544,2:2445)]

# filter NAs
ml_EC_16S_ROOT <- ml_EC_16S_ROOT %>%
  filter(!is.na(root))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/ROOT_model_results/ml_EC_16S_ROOT_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(4)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_ROOT, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting root rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(root ~ ., data = ml_EC_16S_ROOT)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 1223, trees = 500, min_n = 7) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/ROOT_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  ROOT_EC_lm <- lm(root ~ .pred, data = test_predictions)
  p1 <- ggplot(ROOT_EC_lm$model, aes(x = root, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(ROOT_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(ROOT_EC_lm)$coef[2,4], 2)),
                       x = "Observed ROOT Rating", y = "Predicted ROOT Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(ROOT_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/ROOT_model_results/ROOT_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(ROOT_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/ROOT_model_results/ROOT_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/ROOT_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_ROOT)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/ROOT_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/ROOT_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict RICH rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_RICH <- ml_EC_16S[,c(2545,2:2445)]

# filter NAs
ml_EC_16S_RICH <- ml_EC_16S_RICH %>%
  filter(!is.na(rich))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/RICH_model_results/ml_EC_16S_RICH_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(5)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_RICH, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting RICH rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(rich ~ ., data = ml_EC_16S_RICH)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/RICH_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  RICH_EC_lm <- lm(rich ~ .pred, data = test_predictions)
  p1 <- ggplot(RICH_EC_lm$model, aes(x = rich, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(RICH_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(RICH_EC_lm)$coef[2,4], 2)),
                       x = "Observed RICH Rating", y = "Predicted RICH Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(RICH_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/RICH_model_results/RICH_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(RICH_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/RICH_model_results/RICH_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/RICH_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_RICH)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/RICH_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/RICH_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict DIST rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_DIST <- ml_EC_16S[,c(2546,2:2445)]

# filter NAs
ml_EC_16S_DIST <- ml_EC_16S_DIST %>%
  filter(!is.na(dist))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/DIST_model_results/ml_EC_16S_DIST_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(6)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_DIST, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting DIST rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(dist ~ ., data = ml_EC_16S_DIST)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 1834, trees = 250, min_n = 5) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/DIST_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  DIST_EC_lm <- lm(dist ~ .pred, data = test_predictions)
  p1 <- ggplot(DIST_EC_lm$model, aes(x = dist, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(DIST_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(DIST_EC_lm)$coef[2,4], 2)),
                       x = "Observed DIST Rating", y = "Predicted DIST Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(DIST_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/DIST_model_results/DIST_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(DIST_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/DIST_model_results/DIST_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/DIST_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_DIST)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/DIST_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/DIST_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict ACE rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_ACE <- ml_EC_16S[,c(2575,2:2445)]

# filter NAs
ml_EC_16S_ACE$ace.corr <- as.numeric(ml_EC_16S_ACE$ace.corr)
ml_EC_16S_ACE <- ml_EC_16S_ACE %>%
  filter(!is.na(ace.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/ACE_model_results/ml_EC_16S_ACE_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(7)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_ACE, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting ACE rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(ace.corr ~ ., data = ml_EC_16S_ACE)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/ACE_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  ACE_EC_lm <- lm(ace.corr ~ .pred, data = test_predictions)
  p1 <- ggplot(ACE_EC_lm$model, aes(x = ace.corr, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(ACE_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(ACE_EC_lm)$coef[2,4], 2)),
                       x = "Observed ACE Rating", y = "Predicted ACE Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(ACE_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/ACE_model_results/ACE_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(ACE_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/ACE_model_results/ACE_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/ACE_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_ACE)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/ACE_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/ACE_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict ACTIVEC rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_ACTIVEC <- ml_EC_16S[,c(2563,2:2445)]

# filter NAs
ml_EC_16S_ACTIVEC$activeC.corr <- as.numeric(ml_EC_16S_ACTIVEC$activeC.corr)
ml_EC_16S_ACTIVEC <- ml_EC_16S_ACTIVEC %>%
  filter(!is.na(activeC.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/ACTIVEC_model_results/ml_EC_16S_ACTIVEC_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(8)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_ACTIVEC, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting ACTIVEC rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(activeC.corr ~ ., data = ml_EC_16S_ACTIVEC)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/ACTIVEC_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  ACTIVEC_EC_lm <- lm(activeC.corr ~ .pred, data = test_predictions)
  p1 <- ggplot(ACTIVEC_EC_lm$model, aes(x = activeC.corr, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(ACTIVEC_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(ACTIVEC_EC_lm)$coef[2,4], 2)),
                       x = "Observed ACTIVEC Rating", y = "Predicted ACTIVEC Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(ACTIVEC_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/ACTIVEC_model_results/ACTIVEC_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(ACTIVEC_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/ACTIVEC_model_results/ACTIVEC_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/ACTIVEC_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_ACTIVEC)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/ACTIVEC_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/ACTIVEC_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict AGGSTAB rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_AGGSTAB <- ml_EC_16S[,c(2572,2:2445)]

# filter NAs
ml_EC_16S_AGGSTAB$agg_stab.corr <- as.numeric(ml_EC_16S_AGGSTAB$agg_stab.corr)
ml_EC_16S_AGGSTAB <- ml_EC_16S_AGGSTAB %>%
  filter(!is.na(agg_stab.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/AGGSTAB_model_results/ml_EC_16S_AGGSTAB_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(9)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_AGGSTAB, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting AGGSTAB rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(agg_stab.corr ~ ., data = ml_EC_16S_AGGSTAB)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 250, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  AGGSTAB_EC_lm <- lm(agg_stab.corr ~ .pred, data = test_predictions)
  p1 <- ggplot(AGGSTAB_EC_lm$model, aes(x = agg_stab.corr, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(AGGSTAB_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(AGGSTAB_EC_lm)$coef[2,4], 2)),
                       x = "Observed AGGSTAB Rating", y = "Predicted AGGSTAB Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(AGGSTAB_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/AGGSTAB_model_results/AGGSTAB_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(AGGSTAB_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/AGGSTAB_model_results/AGGSTAB_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_AGGSTAB)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/AGGSTAB_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict RESP rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_RESP <- ml_EC_16S[,c(2566,2:2445)]

# filter NAs
ml_EC_16S_RESP$resp.corr <- as.numeric(ml_EC_16S_RESP$resp.corr)
ml_EC_16S_RESP <- ml_EC_16S_RESP %>%
  filter(!is.na(resp.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/RESP_model_results/ml_EC_16S_RESP_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(10)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_RESP, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting RESP rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(resp.corr ~ ., data = ml_EC_16S_RESP)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 7) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/RESP_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  RESP_EC_lm <- lm(resp.corr ~ .pred, data = test_predictions)
  p1 <- ggplot(RESP_EC_lm$model, aes(x = resp.corr, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(RESP_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(RESP_EC_lm)$coef[2,4], 2)),
                       x = "Observed RESP Rating", y = "Predicted RESP Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(RESP_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/RESP_model_results/RESP_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(RESP_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/RESP_model_results/RESP_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/RESP_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_RESP)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/RESP_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/RESP_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict SOM rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_SOM <- ml_EC_16S[,c(2560,2:2445)]

# filter NAs
ml_EC_16S_SOM$SOM.corr <- as.numeric(ml_EC_16S_SOM$SOM.corr)
ml_EC_16S_SOM <- ml_EC_16S_SOM %>%
  filter(!is.na(SOM.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/SOM_model_results/ml_EC_16S_SOM_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

  set.seed(11)
  
  for (i in 1:25) {
    soil_split <- initial_split(ml_EC_16S_SOM, prop = 4/5)
    soil_split
    
    # extract the train and test sets
    soil_train <- training(soil_split)
    soil_test <- testing(soil_split)
    
    # cross validation
    soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
    
    # define a "recipe", i.e., the role of each variable in the model
    # predicting SOM rating, and all other variables (ECs) are predictors
    soil_recipe <- recipe(SOM.corr ~ ., data = ml_EC_16S_SOM)
    soil_recipe
    
    # specify the model
    rf_model <- rand_forest() %>% 
      set_args(mtry = 611, trees = 500, min_n = 7) %>%
      set_engine("ranger", importance = "impurity") %>%
      set_mode("regression")
    
    # set the workflow
    rf_workflow <- workflow() %>%
      add_recipe(soil_recipe) %>%
      add_model(rf_model)
    
    # fit the model
    rf_fit <- rf_workflow %>%
      last_fit(soil_split)
    rf_fit
    
    # save the fit
    saveRDS(rf_fit, paste("machine_learning/16S_EC/SOM_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
    
    # see how well the model performs
    test_performance <- rf_fit %>% collect_metrics()
    test_performance
    
    # generate predictions from the test set
    test_predictions <- rf_fit %>% collect_predictions()
    test_predictions
    
    # graph a regression of predicted vs observed SH_rating values
    SOM_EC_lm <- lm(SOM.corr ~ .pred, data = test_predictions)
    p1 <- ggplot(SOM_EC_lm$model, aes(x = SOM.corr, y = .pred)) +
      geom_point() +
      stat_smooth(method = "lm", se = TRUE, level = 0.95) +
      labs(title = paste("Adj R2 =",signif(summary(SOM_EC_lm)$adj.r.squared, 2),
                         " P =",signif(summary(SOM_EC_lm)$coef[2,4], 2)),
                         x = "Observed SOM Rating", y = "Predicted SOM Rating") +
      theme_bw()
    p1
    
    # save R^2 and p-values to files
    write.table(summary(SOM_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/SOM_model_results/SOM_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
    
    write.table(summary(SOM_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/SOM_model_results/SOM_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
    
    # save plot
    ggsave(paste("machine_learning/16S_EC/SOM_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
    
    # fitting the final model
    # uses all data that can be tested on a new data set
    final_model <- fit(rf_workflow, ml_EC_16S_SOM)
    
    # save the final model
    saveRDS(final_model, paste("machine_learning/16S_EC/SOM_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
    
    # variable importance
    ranger_obj <- pull_workflow_fit(final_model)$fit
    ranger_obj
    var_importance <- as.data.frame(ranger_obj$variable.importance)
    write.csv(var_importance, paste("machine_learning/16S_EC/SOM_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
  }
```

### Predict WATERCAP rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
```{r}
ml_EC_16S <- readRDS("machine_learning/16S_EC/ml_EC_16S.RDS")

ml_EC_16S_WATERCAP <- ml_EC_16S[,c(2578,2:2445)]

# filter NAs
ml_EC_16S_WATERCAP$water_cap.corr <- as.numeric(ml_EC_16S_WATERCAP$water_cap.corr)
ml_EC_16S_WATERCAP <- ml_EC_16S_WATERCAP %>%
  filter(!is.na(water_cap.corr))

# read in tuning results (generated in individual scripts in scripts/16S_EC, run on Scinet, and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/16S_EC/WATERCAP_model_results/ml_EC_16S_WATERCAP_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(12)

for (i in 1:25) {
  soil_split <- initial_split(ml_EC_16S_WATERCAP, prop = 4/5)
  soil_split
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting WATERCAP rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(water_cap.corr ~ ., data = ml_EC_16S_WATERCAP)
  soil_recipe
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 611, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/16S_EC/WATERCAP_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  WATERCAP_EC_lm <- lm(water_cap.corr ~ .pred, data = test_predictions)
  p1 <- ggplot(WATERCAP_EC_lm$model, aes(x = water_cap.corr, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(WATERCAP_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(WATERCAP_EC_lm)$coef[2,4], 2)),
                       x = "Observed WATERCAP Rating", y = "Predicted WATERCAP Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(WATERCAP_EC_lm)$adj.r.squared, file = "machine_learning/16S_EC/WATERCAP_model_results/WATERCAP_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(WATERCAP_EC_lm)$coef[2,4], file = "machine_learning/16S_EC/WATERCAP_model_results/WATERCAP_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/16S_EC/WATERCAP_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, ml_EC_16S_WATERCAP)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/16S_EC/WATERCAP_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/16S_EC/WATERCAP_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```


