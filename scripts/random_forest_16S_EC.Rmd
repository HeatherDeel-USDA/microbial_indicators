---
title: "Random Forest"
author: "Heather Deel"
date: "2023-08-07"
output: html_document
---

### Setup and packages
```{r setup, include=FALSE}
library(tidymodels)
library(tidyverse)
library(workflows)
library(tune)
library(metagMisc)
library(ranger)
library(randomForest)
```

### Load and format data
# only need to run this once - can skip to the next chunk
# Run in scinet
```{r}
### Hops.2018
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_Hops.2018/EC_metagenome_out/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_Hops.2018/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# get other metadata
data.pred.SHMI <- readRDS('/project/soil_micro_lab/micro_indicators/machine_learning/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('/project/soil_micro_lab/micro_indicators/machine_learning/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# get qPCR data into func
# need to read in raw qPCR values
qPCR_data <- read.delim('/project/soil_micro_lab/micro_indicators/machine_learning/SHAI.Meta.22March2023.q2.qPCR.txt', sep = '\t')
qPCR_data <- qPCR_data %>% 
  filter(!is.na(qPCR))

func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$gene_counts <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hops2018_all <- merge(data.merged, otu, by = "SampleID")
hops2018_all <- as.data.frame(hops2018_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hops2018_ml <- hops2018_all[,c(1,95,152:2390)]

saveRDS(hops2018_ml, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/hops2018_ml_EC.RDS")

### Hops.ARS
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_Hops.ARS/EC_metagenome_out/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_Hops.ARS/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# add qPCR data to new func object
func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$gene_counts <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
hopsARS_all <- merge(data.merged, otu, by = "SampleID")
hopsARS_all <- as.data.frame(hopsARS_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
hopsARS_ml <- hopsARS_all[,c(1,95,152:2413)]

saveRDS(hopsARS_ml, '/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/hopsARS_ml_EC.RDS')

### NRCS
# split NRCS data into two to keep R from crashing
func1 <- readRDS("/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_NRCS/EC_metagenome_out/pred_metagenome_contrib1.RDS")
func2 <- readRDS("/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_NRCS/EC_metagenome_out/pred_metagenome_contrib2.RDS")
df <- read.delim('/project/soil_micro_lab/micro_indicators/2023_0621_picrust2_full/picrust2_out_NRCS/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func1 <- func1 %>%
  full_join(df, by="taxon")
func2 <- func2 %>%
  full_join(df, by="taxon")

# add qPCR data to new func object
func1 <- merge(func1, qPCR_data, by = "sample")
func2 <- merge(func2, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func1$gene_counts <- func1$taxon_rel_abun / 100 * func1$genome_function_count / func1$genome_16S_count * func1$qPCR
func2$gene_counts <- func2$taxon_rel_abun / 100 * func2$genome_function_count / func2$genome_16S_count * func2$qPCR

# change column name "function" to "EC"
colnames(func1)[2] <- "EC"
colnames(func2)[2] <- "EC"

# pool data by sample and function
otu1 <- func1 %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu1 <- otu1 %>%
  spread(key=sample, value=sum)
otu1 <- data.frame(otu1)
otu1[is.na(otu1)] <- 0
row.names(otu1) <- otu1$EC
otu1 <- otu1[,-1]

otu2 <- func2 %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu2 <- otu2 %>%
  spread(key=sample, value=sum)
otu2 <- data.frame(otu2)
otu2[is.na(otu2)] <- 0
row.names(otu2) <- otu2$EC
otu2 <- otu2[,-1]

# transpose the "otu" table
otu1 <- t(otu1)
otu2 <- t(otu2)

# convert matrix to data frame
otu1 <- as.data.frame(otu1)
otu2 <- as.data.frame(otu2)

# make the sample names a column for merging later
otu1 <- tibble::rownames_to_column(otu1, "SampleID")
otu2 <- tibble::rownames_to_column(otu2, "SampleID")

# merge metadata and "otu" table
NRCS1 <- merge(data.merged, otu1, by = "SampleID")
NRCS1 <- as.data.frame(NRCS1)
NRCS2 <- merge(data.merged, otu2, by = "SampleID")
NRCS2 <- as.data.frame(NRCS2)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
NRCS1_ml <- NRCS1[,c(1,95,152:2530)]
NRCS2_ml <- NRCS2[,c(1,95,152:2468)]

saveRDS(NRCS1_ml, "machine_learning/NRCS1_ml_EC.RDS")
saveRDS(NRCS2_ml, "machine_learning/NRCS2_ml_EC.RDS")

### Rangeland
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('../picrust2_files/Rangeland/EC/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('../picrust2_files/Rangeland/marker_predicted_and_nsti.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_16S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# add qPCR data to new func object
func <- merge(func, qPCR_data, by = "sample")

# we imported un-normalized counts, so normalizing here
func$gene_counts <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_16S_count * func$qPCR

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
rangeland_all <- merge(data.merged, otu, by = "SampleID")
rangeland_all <- as.data.frame(rangeland_all)

# select columns to be used in ml - SampleID, ECs, and "Overall" CASH rating
rangeland_ml <- rangeland_all[,c(1,95,152:2391)]

saveRDS(rangeland_ml, "machine_learning/rangeland_ml_EC.RDS")

### final formatting of ml_all object

# merge ml dfs for Hops.2018 Hops.ARS, NRCS1, NRCS2, and Rangeland

ml_1 <- merge(hops2018_ml, hopsARS_ml, all = TRUE)
ml_2 <- merge(NRCS1_ml, ml_1, all = TRUE)
ml_3 <- merge(NRCS2_ml, ml_2, all = TRUE)
ml_all <- merge(rangeland_ml, ml_3, all = TRUE)

# going to assume that NAs are 0 (functions not found in one data set or another)
# change NAs to 0
ml_all[is.na(ml_all)] <- 0

# NRCS 346 duplicated for some reason. One row is all zeros. Get rid of row 316
ml_all <- ml_all[c(1:315,317:537),]

# make the sample names the row names again to make selecting columns for ml easy
rownames(ml_all) <- ml_all$SampleID

# get rid of sampleID column
ml_all <- ml_all[,c(2:2444)]

# check for any NAs in ml_all
sum(is.na(ml_all))
# none

# data is ready for ML

# save ML object
saveRDS(ml_all, "machine_learning/ml_all_EC.RDS")
```

### Predict overall CASH rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
# Run in Ceres on Demand due to high computational requirements
```{r}
# reload ML object
ml_all <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_EC.RDS")

soil_split <- initial_split(ml_all, prop = 4/5)
soil_split
# 428 samples in training, 108 in testing, 536 total

# extract the train and test sets
soil_train <- training(soil_split)
soil_test <- testing(soil_split)

# cross validation
soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)

# define a "recipe", i.e., the role of each variable in the model
# predicting overall CASH rating, and all other variables (ECs) are predictors
# if pre-processing steps or checks are needed (e.g., normalization), can use ?selections to see what selectors can be used

soil_recipe <- recipe(Overall ~ ., data = ml_all)
soil_recipe
# 2442 predictors

# specify the model
# tune randomness, number of trees, and min nodes/max depth (# of splits each decision tree can make)
# set_args values = tune() when tuning
# change set_args parameters to tuned grid values after tuning
rf_model <- rand_forest() %>% 
  set_args(mtry = 1834, trees = 500, min_n = 3) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# set the workflow
rf_workflow <- workflow() %>%
  add_recipe(soil_recipe) %>%
  add_model(rf_model)

# tune the parameters
# comment out section between ########## when not tuning
##########
# doing 25%, 50%, and 75% of the total number of predictors, respectively
#rf_grid <- expand.grid(mtry = c(611,1223,1834),
#                       trees = c(100,250,500),
#                       min_n = c(3,5,7))

# extract results
#rf_tune_results <- rf_workflow %>%
#  tune_grid(resamples = soil_cv, grid = rf_grid, metrics = metric_set(mae, rmse))

# save tune results
#saveRDS(rf_tune_results, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/CASH_model_results/ml_all_EC_CASH_tune_results.RDS")

#rf_tune_results %>%
#  collect_metrics()

# finalize the workflow
#param_final <- rf_tune_results %>%
#  select_best(metric = "mae")
#param_final

#rf_workflow <- rf_workflow %>%
#  finalize_workflow(param_final)
##########

# fit the model
# this will fit on the training set and evaluate on test set
rf_fit <- rf_workflow %>%
  last_fit(soil_split)
rf_fit

# save the fit
saveRDS(rf_fit, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/CASH_model_results/ml_all_EC_fit25.RDS")

# see how well the model performs
test_performance <- rf_fit %>% collect_metrics()
test_performance

# generate predictions from the test set
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

# graph a regression of predicted vs observed SH_rating values
CASH_EC_lm <- lm(Overall ~ .pred, data = test_predictions)
p1 <- ggplot(CASH_EC_lm$model, aes(x = Overall, y = .pred)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, level = 0.95) +
  labs(title = paste("Adj R2 =",signif(summary(CASH_EC_lm)$adj.r.squared, 2),
                     " P =",signif(summary(CASH_EC_lm)$coef[2,4], 2)),
                     x = "Observed CASH Rating", y = "Predicted CASH Rating") +
  theme_bw()
p1

# save plot
ggsave("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/CASH_model_results/ml_all_EC_pred_vs_obs25.tiff", unit = "in", width = 6, height = 6, 
       dpi = 300, device = "tiff")

# fitting the final model
# uses all data that can be tested on a new data set
final_model <- fit(rf_workflow, ml_all)

# save the final model
saveRDS(final_model, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/CASH_model_results/ml_all_EC_final_model25.RDS")

# variable importance
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
var_importance <- as.data.frame(ranger_obj$variable.importance)
write.csv(var_importance, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/CASH_model_results/var_importance_EC25.csv", row.names = TRUE)
```

### Predict SEMWISE rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
# Run in Ceres on Demand due to high computational requirements
```{r}
# in hindsight it would've made more sense to keep all columns in ml_all and subset when I didn't need it
# going to save an object here that has all of the columns for future subsetting
ml_all <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_EC.RDS")

# remake data.merged
data.pred.SHMI <- readRDS('/project/soil_micro_lab/micro_indicators/machine_learning/data.pred.SHMI.RDS')

# read in metadata that includes sampleIDs to merge with data.pred.SHMI
data.sampleID <- read.delim('/project/soil_micro_lab/micro_indicators/machine_learning/SHAI.Meta.22March2023.q2.reduced.txt', sep = '\t')

# merge metadata files
data.merged <- merge(data.pred.SHMI, data.sampleID, by = "PLFA_ID", all.x = TRUE)

# move the sample ID column to the front
data.merged <- data.merged %>% 
  relocate(SampleID)

# rownames to column in ml_all for merging
ml_all2 <- tibble::rownames_to_column(ml_all, "SampleID")

# merge ml_all2 with data.merged
ml_all_meta <- merge(ml_all2, data.merged, by = "SampleID")
rownames(ml_all_meta) <- ml_all_meta$SampleID
ml_all_meta <- ml_all_meta[,c(3:2594)]
colnames(ml_all_meta)[2536] = "Overall"

saveRDS(ml_all_meta, '/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_meta.RDS')
# can now use this object and subset to any columns for ml

ml_all_SEMWISE <- ml_all_meta[,c(1:2442,2554)]

soil_split <- initial_split(ml_all_SEMWISE, prop = 4/5)
soil_split
# 428 samples in training, 108 in testing, 536 total

# extract the train and test sets
soil_train <- training(soil_split)
soil_test <- testing(soil_split)

# cross validation
soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)

# define a "recipe", i.e., the role of each variable in the model
# predicting SEMWISE rating, and all other variables (ECs) are predictors
# if pre-processing steps or checks are needed (e.g., normalization), can use ?selections to see what selectors can be used

soil_recipe <- recipe(SH_rating ~ ., data = ml_all_SEMWISE)
soil_recipe
# 2442 predictors

# specify the model
# tune randomness, number of trees, and min nodes/max depth (# of splits each decision tree can make)
# set_args values = tune() when tuning
# change set_args parameters to tuned grid values after tuning
rf_model <- rand_forest() %>% 
  set_args(mtry = 1223, trees = 250, min_n = 3) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# set the workflow
rf_workflow <- workflow() %>%
  add_recipe(soil_recipe) %>%
  add_model(rf_model)

# tune the parameters
# comment out section between ########## when not tuning
##########
# doing 25%, 50%, and 75% of the total number of predictors, respectively
#rf_grid <- expand.grid(mtry = c(611,1223,1834),
#                       trees = c(100,250,500),
#                       min_n = c(3,5,7))

# extract results
#rf_tune_results <- rf_workflow %>%
#  tune_grid(resamples = soil_cv, grid = rf_grid, metrics = metric_set(mae, rmse))

# save tune results
#saveRDS(rf_tune_results, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_SEMWISE_tune_results.RDS")

#rf_tune_results %>%
#  collect_metrics()

# finalize the workflow
#param_final <- rf_tune_results %>%
#  select_best(metric = "mae")
#param_final

#rf_workflow <- rf_workflow %>%
#  finalize_workflow(param_final)
##########

# fit the model
# this will fit on the training set and evaluate on test set
rf_fit <- rf_workflow %>%
  last_fit(soil_split)
rf_fit

# save the fit
saveRDS(rf_fit, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_fit25.RDS")

# see how well the model performs
test_performance <- rf_fit %>% collect_metrics()
test_performance

# generate predictions from the test set
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

# graph a regression of predicted vs observed SH_rating values
SEMWISE_EC_lm <- lm(SH_rating ~ .pred, data = test_predictions)
p1 <- ggplot(SEMWISE_EC_lm$model, aes(x = SH_rating, y = .pred)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, level = 0.95) +
  labs(title = paste("Adj R2 =",signif(summary(SEMWISE_EC_lm)$adj.r.squared, 2),
                     " P =",signif(summary(SEMWISE_EC_lm)$coef[2,4], 2)),
                     x = "Observed SEMWISE Rating", y = "Predicted SEMWISE Rating") +
  theme_bw()
p1

# save plot
ggsave("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_pred_vs_obs25.tiff", unit = "in", width = 6, height = 6, 
       dpi = 300, device = "tiff")

# fitting the final model
# uses all data that can be tested on a new data set
final_model <- fit(rf_workflow, ml_all_SEMWISE)

# save the final model
saveRDS(final_model, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SEMWISE_model_results/ml_all_EC_final_model25.RDS")

# variable importance
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
var_importance <- as.data.frame(ranger_obj$variable.importance)
write.csv(var_importance, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SEMWISE_model_results/var_importance_EC25.csv", row.names = TRUE)
```

### Predict SHMI rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
# Run in Ceres on Demand due to high computational requirements
```{r}
ml_all_meta <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_meta.RDS")

ml_all_SHMI <- ml_all_meta[,c(1:2442,2591)]

soil_split <- initial_split(ml_all_SHMI, prop = 4/5)
soil_split
# 428 samples in training, 108 in testing, 536 total

# extract the train and test sets
soil_train <- training(soil_split)
soil_test <- testing(soil_split)

# cross validation
soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)

# define a "recipe", i.e., the role of each variable in the model
# predicting SHMI2_rating, and all other variables (ECs) are predictors
# if pre-processing steps or checks are needed (e.g., normalization), can use ?selections to see what selectors can be used

soil_recipe <- recipe(SHMI2_rating ~ ., data = ml_all_SHMI)
soil_recipe
# 2442 predictors

# specify the model
# tune randomness, number of trees, and min nodes/max depth (# of splits each decision tree can make)
# set_args values = tune() when tuning
# change set_args parameters to tuned grid values after tuning
rf_model <- rand_forest() %>% 
  set_args(mtry = 1223, trees = 500, min_n = 3) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# set the workflow
rf_workflow <- workflow() %>%
  add_recipe(soil_recipe) %>%
  add_model(rf_model)

# tune the parameters
# comment out section between ########## when not tuning
##########
# doing 25%, 50%, and 75% of the total number of predictors, respectively
rf_grid <- expand.grid(mtry = c(611,1223,1834),
                       trees = c(100,250,500),
                       min_n = c(3,5,7))

# extract results
rf_tune_results <- rf_workflow %>%
  tune_grid(resamples = soil_cv, grid = rf_grid, metrics = metric_set(mae, rmse))

# save tune results
saveRDS(rf_tune_results, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/ml_all_EC_SHMI_tune_results.RDS")

rf_tune_results <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/ml_all_EC_SHMI_tune_results.RDS")

rf_tune_results %>%
  collect_metrics()

# finalize the workflow
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

rf_workflow <- rf_workflow %>%
  finalize_workflow(param_final)
##########

# fit the model
# this will fit on the training set and evaluate on test set
rf_fit <- rf_workflow %>%
  last_fit(soil_split)
rf_fit

# save the fit
saveRDS(rf_fit, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/ml_all_EC_fit.RDS")

# see how well the model performs
test_performance <- rf_fit %>% collect_metrics()
test_performance

# generate predictions from the test set
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

# graph a regression of predicted vs observed SH_rating values
SHMI_EC_lm <- lm(SHMI2_rating ~ .pred, data = test_predictions)
p1 <- ggplot(SHMI_EC_lm$model, aes(x = SHMI2_rating, y = .pred)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, level = 0.95) +
  labs(title = paste("Adj R2 =",signif(summary(SHMI_EC_lm)$adj.r.squared, 2),
                     " P =",signif(summary(SHMI_EC_lm)$coef[2,4], 2)),
                     x = "Observed SHMI Rating", y = "Predicted SHMI Rating") +
  theme_bw()
p1

# save plot
ggsave("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/ml_all_EC_pred_vs_obs.tiff", unit = "in", width = 6, height = 6, 
       dpi = 300, device = "tiff")

# fitting the final model
# uses all data that can be tested on a new data set
final_model <- fit(rf_workflow, ml_all_SHMI)

# save the final model
saveRDS(final_model, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/ml_all_EC_final_model.RDS")

# variable importance
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
var_importance <- as.data.frame(ranger_obj$variable.importance)
write.csv(var_importance, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SHMI_model_results/var_importance_EC.csv", row.names = TRUE)
```

### Predict organic matter rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
# Run in Ceres on Demand due to high computational requirements
```{r}
ml_all_meta <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_meta.RDS")

ml_all_SOM <- ml_all_meta[,c(1:2442,2518)]

soil_split <- initial_split(ml_all_SOM, prop = 4/5)
soil_split
# 428 samples in training, 108 in testing, 536 total

# extract the train and test sets
soil_train <- training(soil_split)
soil_test <- testing(soil_split)

# cross validation
soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)

# define a "recipe", i.e., the role of each variable in the model
# predicting SOM rating, and all other variables (ECs) are predictors
# if pre-processing steps or checks are needed (e.g., normalization), can use ?selections to see what selectors can be used

soil_recipe <- recipe(organic_matter_rating ~ ., data = ml_all_SOM)
soil_recipe
# 2442 predictors

# specify the model
# tune randomness, number of trees, and min nodes/max depth (# of splits each decision tree can make)
# set_args values = tune() when tuning
# change set_args parameters to tuned grid values after tuning
rf_model <- rand_forest() %>% 
  set_args(mtry = 1834, trees = 500, min_n = 3) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# set the workflow
rf_workflow <- workflow() %>%
  add_recipe(soil_recipe) %>%
  add_model(rf_model)

# tune the parameters
# comment out section between ########## when not tuning
##########
# doing 25%, 50%, and 75% of the total number of predictors, respectively
#rf_grid <- expand.grid(mtry = c(611,1223,1834),
#                       trees = c(100,250,500),
#                       min_n = c(3,5,7))

# extract results
#rf_tune_results <- rf_workflow %>%
#  tune_grid(resamples = soil_cv, grid = rf_grid, metrics = metric_set(mae, rmse))

# save tune results
#saveRDS(rf_tune_results, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SOM_model_results/ml_all_EC_SOM_tune_results.RDS")

#rf_tune_results %>%
#  collect_metrics()

# finalize the workflow
#param_final <- rf_tune_results %>%
#  select_best(metric = "mae")
#param_final

#rf_workflow <- rf_workflow %>%
#  finalize_workflow(param_final)
##########

# fit the model
# this will fit on the training set and evaluate on test set
rf_fit <- rf_workflow %>%
  last_fit(soil_split)
rf_fit

# save the fit
saveRDS(rf_fit, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SOM_model_results/ml_all_EC_fit25.RDS")

# see how well the model performs
test_performance <- rf_fit %>% collect_metrics()
test_performance

# generate predictions from the test set
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

# graph a regression of predicted vs observed values
SOM_EC_lm <- lm(organic_matter_rating ~ .pred, data = test_predictions)
p1 <- ggplot(SOM_EC_lm$model, aes(x = organic_matter_rating, y = .pred)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, level = 0.95) +
  labs(title = paste("Adj R2 =",signif(summary(SOM_EC_lm)$adj.r.squared, 2),
                     " P =",signif(summary(SOM_EC_lm)$coef[2,4], 2)),
                     x = "Observed SOM Rating", y = "Predicted SOM Rating") +
  theme_bw()
p1

# save plot
ggsave("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SOM_model_results/ml_all_EC_pred_vs_obs25.tiff", unit = "in", width = 6, height = 6, 
       dpi = 300, device = "tiff")

# fitting the final model
# uses all data that can be tested on a new data set
final_model <- fit(rf_workflow, ml_all_SOM)

# save the final model
saveRDS(final_model, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SOM_model_results/ml_all_EC_final_model25.RDS")

# variable importance
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
var_importance <- as.data.frame(ranger_obj$variable.importance)
write.csv(var_importance, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/SOM_model_results/var_importance_EC25.csv", row.names = TRUE)
```

### Predict aggregate stability rating
# Run 25 times, with outputs 1-25, to account for lucky/unlucky splits after tuning
# Run in Ceres on Demand due to high computational requirements
```{r}
ml_all_meta <- readRDS("/project/soil_micro_lab/micro_indicators/machine_learning/ml_all_meta.RDS")

ml_all_AGGSTAB <- ml_all_meta[,c(1:2442,2516)]

soil_split <- initial_split(ml_all_AGGSTAB, prop = 4/5)
soil_split
# 428 samples in training, 108 in testing, 536 total

# extract the train and test sets
soil_train <- training(soil_split)
soil_test <- testing(soil_split)

# cross validation
soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)

# define a "recipe", i.e., the role of each variable in the model
# predicting aggregate stability rating, and all other variables (ECs) are predictors
# if pre-processing steps or checks are needed (e.g., normalization), can use ?selections to see what selectors can be used

soil_recipe <- recipe(aggregate_stability_rating ~ ., data = ml_all_AGGSTAB)
soil_recipe
# 2442 predictors

# specify the model
# tune randomness, number of trees, and min nodes/max depth (# of splits each decision tree can make)
# set_args values = tune() when tuning
# change set_args parameters to tuned grid values after tuning
rf_model <- rand_forest() %>% 
  set_args(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# set the workflow
rf_workflow <- workflow() %>%
  add_recipe(soil_recipe) %>%
  add_model(rf_model)

# tune the parameters
# comment out section between ########## when not tuning
##########
# doing 25%, 50%, and 75% of the total number of predictors, respectively
rf_grid <- expand.grid(mtry = c(611,1223,1834),
                       trees = c(100,250,500),
                       min_n = c(3,5,7))

# extract results
rf_tune_results <- rf_workflow %>%
  tune_grid(resamples = soil_cv, grid = rf_grid, metrics = metric_set(mae, rmse))

# save tune results
saveRDS(rf_tune_results, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_AGGSTAB_tune_results.RDS")

rf_tune_results %>%
  collect_metrics()

# finalize the workflow
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

rf_workflow <- rf_workflow %>%
  finalize_workflow(param_final)
##########

# fit the model
# this will fit on the training set and evaluate on test set
rf_fit <- rf_workflow %>%
  last_fit(soil_split)
rf_fit

# save the fit
saveRDS(rf_fit, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_fit.RDS")

# see how well the model performs
test_performance <- rf_fit %>% collect_metrics()
test_performance

# generate predictions from the test set
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

# graph a regression of predicted vs observed SH_rating values
AGGSTAB_EC_lm <- lm(aggregate_stability_rating ~ .pred, data = test_predictions)
p1 <- ggplot(AGGSTAB_EC_lm$model, aes(x = aggregate_stability_rating, y = .pred)) +
  geom_point() +
  stat_smooth(method = "lm", se = TRUE, level = 0.95) +
  labs(title = paste("Adj R2 =",signif(summary(AGGSTAB_EC_lm)$adj.r.squared, 2),
                     " P =",signif(summary(AGGSTAB_EC_lm)$coef[2,4], 2)),
                     x = "Observed Aggregate Stability Rating", y = "Predicted Aggregate Stability Rating") +
  theme_bw()
p1

# save plot
ggsave("/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_pred_vs_obs.tiff", unit = "in", width = 6, height = 6, 
       dpi = 300, device = "tiff")

# fitting the final model
# uses all data that can be tested on a new data set
final_model <- fit(rf_workflow, ml_all_AGGSTAB)

# save the final model
saveRDS(final_model, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/AGGSTAB_model_results/ml_all_EC_final_model.RDS")

# variable importance
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
var_importance <- as.data.frame(ranger_obj$variable.importance)
write.csv(var_importance, "/project/soil_micro_lab/micro_indicators/machine_learning/16S_EC/AGGSTAB_model_results/var_importance_EC.csv", row.names = TRUE)
```



