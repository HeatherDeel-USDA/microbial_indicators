---
title: "18S_EC"
author: "Heather Deel"
date: "2023-09-22"
output: html_document
---

### Setup and packages
```{r setup, include=FALSE}
library(tidymodels)
library(tidyverse)
library(workflows)
library(tune)
library(metagMisc)
library(ranger)
library(randomForest)
```

### Load and format data
# only need to run this once - can skip to the next chunk
# run on local computer
```{r}
# we don't need the full phyloseq from before, so reimporting
func <- read.delim('picrust2_files/18S/EC_metagenome_out/pred_metagenome_contrib.tsv.gz', sep='\t')
df <- read.delim('picrust2_files/18S/picrust2_out_SHAI_18S/marker_nsti_predicted.tsv.gz', sep = '\t')

names(df) <- c("taxon", "genome_18S_count", "metadata_NSTI")

# combine two files
func <- func %>%
  full_join(df, by="taxon")

# read in metadata
data.18S <- read.delim('metadata/SHAI.Meta.6September2023.q2.18S.txt', sep = '\t')

# get qPCR data into func
# subset data.18S to just SampleID and qPCR_18S
qPCR_18S_df <- data.18S[,c(1,3)]
colnames(qPCR_18S_df)[1] <- "sample"
func <- merge(func, qPCR_18S_df, by = "sample")
func <- func %>% 
  filter(!is.na(qPCR_18S))

# we imported un-normalized counts, so normalizing here
func$gene_counts <- func$taxon_rel_abun / 100 * func$genome_function_count / func$genome_18S_count * func$qPCR_18S

# change column name "function" to "EC"
colnames(func)[2] <- "EC"

# pool data by sample and function
otu <- func %>%
  group_by(sample, EC) %>%
  summarize(sum=sum(gene_counts))
otu <- otu %>%
  spread(key=sample, value=sum)
otu <- data.frame(otu)
otu[is.na(otu)] <- 0
row.names(otu) <- otu$EC
otu <- otu[,-1]

# transpose the "otu" table
otu <- t(otu)

# convert matrix to data frame
otu <- as.data.frame(otu)

# make the sample names a column for merging later
otu <- tibble::rownames_to_column(otu, "SampleID")

# merge metadata and "otu" table
SHAI18S_all <- merge(data.18S, otu, by = "SampleID")
SHAI18S_ml <- as.data.frame(SHAI18S_all)

saveRDS(SHAI18S_ml, "machine_learning/18S_EC/SHAI18S_ml_EC.RDS")
```

### Predict Overall CASH rating
# Includes loop for 1-25 models to account for lucky/unlucky splits
# Run locally
```{r}
SHAI18S_ml <- readRDS("machine_learning/18S_EC/SHAI18S_ml_EC.RDS")

SHAI18S_ml_CASH <- SHAI18S_ml[,c(97,154:1282)]

# filter NAs
SHAI18S_ml_CASH <- SHAI18S_ml_CASH %>%
  filter(!is.na(Overall))

# read in tuning results (generated in random_forest_18S_EC_tuning.Rmd on Scinet and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/18S_EC/CASH_model_results/SHAI18S_ml_CASH_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(1)

for (i in 1:25) {
  soil_split <- initial_split(SHAI18S_ml_CASH, prop = 4/5)
  soil_split
  # 252 samples in training, 64 in testing, 316 total
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting overall CASH rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(Overall ~ ., data = SHAI18S_ml_CASH)
  soil_recipe
  # 1129 predictors
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 282, trees = 500, min_n = 5) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/18S_EC/CASH_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  CASH_EC_lm <- lm(Overall ~ .pred, data = test_predictions)
  p1 <- ggplot(CASH_EC_lm$model, aes(x = Overall, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(CASH_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(CASH_EC_lm)$coef[2,4], 2)),
                       x = "Observed CASH Rating", y = "Predicted CASH Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(CASH_EC_lm)$adj.r.squared, file = "machine_learning/18S_EC/CASH_model_results/CASH_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(CASH_EC_lm)$coef[2,4], file = "machine_learning/18S_EC/CASH_model_results/CASH_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/18S_EC/CASH_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, SHAI18S_ml_CASH)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/18S_EC/CASH_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/18S_EC/CASH_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict SEMWISE rating
# Includes loop for 1-25 models to account for lucky/unlucky splits
# Run locally
```{r}
SHAI18S_ml <- readRDS("machine_learning/18S_EC/SHAI18S_ml_EC.RDS")

SHAI18S_ml_SEMWISE <- SHAI18S_ml[,c(115,154:1282)]

# filter NAs
SHAI18S_ml_SEMWISE <- SHAI18S_ml_SEMWISE %>%
  filter(!is.na(SH_rating))

# read in tuning results (generated in random_forest_18S_EC_tuning.Rmd on Scinet and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/18S_EC/SEMWISE_model_results/SHAI18S_ml_SEMWISE_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(2)

for (i in 1:25) {
  soil_split <- initial_split(SHAI18S_ml_SEMWISE, prop = 4/5)
  soil_split
  # 252 samples in training, 64 in testing, 316 total
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting SH_rating SEMWISE rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(SH_rating ~ ., data = SHAI18S_ml_SEMWISE)
  soil_recipe
  # 1129 predictors
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 847, trees = 500, min_n = 5) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/18S_EC/SEMWISE_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SH_rating values
  SEMWISE_EC_lm <- lm(SH_rating ~ .pred, data = test_predictions)
  p1 <- ggplot(SEMWISE_EC_lm$model, aes(x = SH_rating, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(SEMWISE_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(SEMWISE_EC_lm)$coef[2,4], 2)),
                       x = "Observed SEMWISE Rating", y = "Predicted SEMWISE Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(SEMWISE_EC_lm)$adj.r.squared, file = "machine_learning/18S_EC/SEMWISE_model_results/SEMWISE_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(SEMWISE_EC_lm)$coef[2,4], file = "machine_learning/18S_EC/SEMWISE_model_results/SEMWISE_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/18S_EC/SEMWISE_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, SHAI18S_ml_SEMWISE)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/18S_EC/SEMWISE_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/18S_EC/SEMWISE_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```

### Predict SHMI rating
# Includes loop for 1-25 models to account for lucky/unlucky splits
# Run locally
```{r}
SHAI18S_ml <- readRDS("machine_learning/18S_EC/SHAI18S_ml_EC.RDS")

SHAI18S_ml_SHMI <- SHAI18S_ml[,c(152,154:1282)]

# filter NAs
SHAI18S_ml_SHMI <- SHAI18S_ml_SHMI %>%
  filter(!is.na(SHMI2_rating))

# read in tuning results (generated in random_forest_18S_EC_tuning.Rmd on Scinet and transferred to local one drive)
# change the set_args parameters before running the 25 loops
rf_tune_results <- readRDS("machine_learning/18S_EC/SHMI_model_results/SHAI18S_ml_SHMI_tune_results.RDS")
param_final <- rf_tune_results %>%
  select_best(metric = "mae")
param_final

set.seed(3)

for (i in 1:25) {
  soil_split <- initial_split(SHAI18S_ml_SHMI, prop = 4/5)
  soil_split
  # 252 samples in training, 64 in testing, 316 total
  
  # extract the train and test sets
  soil_train <- training(soil_split)
  soil_test <- testing(soil_split)
  
  # cross validation
  soil_cv <- vfold_cv(soil_train, v = 5, repeats = 10, strata = NULL)
  
  # define a "recipe", i.e., the role of each variable in the model
  # predicting SHMI rating, and all other variables (ECs) are predictors
  soil_recipe <- recipe(SHMI2_rating ~ ., data = SHAI18S_ml_SHMI)
  soil_recipe
  # 1129 predictors
  
  # specify the model
  rf_model <- rand_forest() %>% 
    set_args(mtry = 847, trees = 500, min_n = 3) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")
  
  # set the workflow
  rf_workflow <- workflow() %>%
    add_recipe(soil_recipe) %>%
    add_model(rf_model)
  
  # fit the model
  rf_fit <- rf_workflow %>%
    last_fit(soil_split)
  rf_fit
  
  # save the fit
  saveRDS(rf_fit, paste("machine_learning/18S_EC/SHMI_model_results/ml_all_EC_fit", i, ".RDS", sep = ""))
  
  # see how well the model performs
  test_performance <- rf_fit %>% collect_metrics()
  test_performance
  
  # generate predictions from the test set
  test_predictions <- rf_fit %>% collect_predictions()
  test_predictions
  
  # graph a regression of predicted vs observed SHMI2_rating values
  SHMI_EC_lm <- lm(SHMI2_rating ~ .pred, data = test_predictions)
  p1 <- ggplot(SHMI_EC_lm$model, aes(x = SHMI2_rating, y = .pred)) +
    geom_point() +
    stat_smooth(method = "lm", se = TRUE, level = 0.95) +
    labs(title = paste("Adj R2 =",signif(summary(SHMI_EC_lm)$adj.r.squared, 2),
                       " P =",signif(summary(SHMI_EC_lm)$coef[2,4], 2)),
                       x = "Observed SHMI Rating", y = "Predicted SHMI Rating") +
    theme_bw()
  p1
  
  # save R^2 and p-values to files
  write.table(summary(SHMI_EC_lm)$adj.r.squared, file = "machine_learning/18S_EC/SHMI_model_results/SHMI_r2_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  write.table(summary(SHMI_EC_lm)$coef[2,4], file = "machine_learning/18S_EC/SHMI_model_results/SHMI_p_values.txt", append = TRUE, sep = "\t", row.names = FALSE, col.names = FALSE)
  
  # save plot
  ggsave(paste("machine_learning/18S_EC/SHMI_model_results/ml_all_EC_pred_vs_obs", i, ".tiff", sep = ""), unit = "in", width = 6, height = 6, dpi = 300, device = "tiff")
  
  # fitting the final model
  # uses all data that can be tested on a new data set
  final_model <- fit(rf_workflow, SHAI18S_ml_SHMI)
  
  # save the final model
  saveRDS(final_model, paste("machine_learning/18S_EC/SHMI_model_results/ml_all_EC_final_model", i, ".RDS", sep = ""))
  
  # variable importance
  ranger_obj <- pull_workflow_fit(final_model)$fit
  ranger_obj
  var_importance <- as.data.frame(ranger_obj$variable.importance)
  write.csv(var_importance, paste("machine_learning/18S_EC/SHMI_model_results/var_importance_EC", i, ".csv", sep = ""), row.names = TRUE)
}
```





